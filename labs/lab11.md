# Lab 11: LLM Security
When deploying systems that include Large Language Models (LLMs), it is important to keep in mind certain vulnerabilities in those systems. Just like in any software system, it is impossible to make LLM apps entirely safe. While attacks on "traditional" systems require advanced technological skills, it is more intuitive and straightforward to exploit LLM applications - after all you can literally just talk to them. 

In this lab you will explore some of the vulnerabilities of LLM systems, and get a feeling for different prompt-based attack and defense options.

## Deliverables
- [ ] Come up with a defense prompt that protect against all the attacks and show your experiments to a TA during recitation.
- [ ] Show your new attack to the TA and explain why you think it broke your defense.
- [ ] Show your safeguard to the TA and explain how it works. Think about other ways to further improve the security of the system.

Clone this [repository](https://github.com/malusamayo/cmu-mlip-lab11) and follow instructions in the notebook `lab11.ipynb`.
